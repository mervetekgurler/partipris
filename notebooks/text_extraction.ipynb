{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8509830b",
   "metadata": {},
   "source": [
    "# Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d5a85",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b441ce",
   "metadata": {},
   "source": [
    "The following notebook details Merve Tekgürler's approach to text extraction from the Parti Pris corpus from Summer 2025.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../img/parti_pris_cover.png\"\n",
    "         alt=\"Cover of Parti Pris's October 1963 issue\">\n",
    "    <figcaption>Cover of <a href=\"https://collections.banq.qc.ca/ark:/52327/2314782\" target=\"_blank\">Parti Pris' inaugural issue</a> from October 1963.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093d87f",
   "metadata": {},
   "source": [
    "In 2023-2024, Chloé Brault, Clare Chua, and Em Ho compiled the Parti Pris corpus and extracted texts from the same PDFs using ABBYY FineReader. More details about their work can be found [here](https://msuglobaldh.org/abstracts/#brault).\n",
    "\n",
    "Tekgürler and Brault revisited the project in Summer 2025. This notebook describes how we used Gemini 2.5 API to extract texts from the Parti Pris corpus. It shared the prompting code and offers some insights into the decisions that went into the designing of the prompts as well as safeguards to verify the quality of the OCR output. We "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d58b3",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eea980",
   "metadata": {},
   "source": [
    "The complete Parti Pris PDF corpus is approximately 3.48GB in size and includes all 42 issues from the magazine’s entire print run, digitized by the Bibliothèque et Archives Nationales du Québec (BAnQ).\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../img/banq.png\"\n",
    "         alt=\"Screenshot showing the search page for Parti Pris on the BAnQ website\">\n",
    "    <figcaption>Searching for Parti Pris on the BAnQ website. You can find all the issues of Parti Pris and download the PDFs by following <a href=\"https://numerique.banq.qc.ca/rechercheExterne/encoded/Kg==/false/D/asc/W3sibm9tIjoiY29ycHVzIiwidmFsZXVyIjoiUGF0cmltb2luZSUyMHF1w6liw6ljb2lzIn0seyJub20iOiJ0eXBlX2RvY19mIiwidmFsZXVyIjoiUmV2dWVzJTIwZXQlMjBqb3VybmF1eCJ9LHsibm9tIjoibnVtZXJvX25vdGljZSIsInZhbGV1ciI6IjAwMDAxNjMxMjIifV0=/Toutes%20les%20ressources/true/false/\" target=\"_blank\">this search link</a>. </figcaption>\n",
    "</figure>\n",
    "\n",
    "There are three example PDFs in the data folder of this repository (../data/full_pdfs and ../data/split_pdfs), for testing the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584e539",
   "metadata": {},
   "source": [
    "## Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d26c84",
   "metadata": {},
   "source": [
    "The previous iteration of this project experimented with OCR'ing entire issues of Parti Pris and then spliting it into small chunks of text for further analysis. This allowed us, for example, to experiment with Named Entity Recognition algorithms to identify places mentioned in this corpus or run word frequency-based analyses to discover trends across the whole corpus. It has however proven to be rather difficult to split the OCR'ed issues into individual articles.\n",
    "\n",
    "In this new iteration of the project, we revisit our approach to OCR. We use the Gemini 2.5 API to extract text and return a JSON object containing each article as a separate entry. \n",
    "\n",
    "### Advantages\n",
    "\n",
    "There are many advantanges to this approach. We automatically obtain texts split at article level associated with the author of that article as opposed to just with Parti Pris. Parti Pris has relatively standard layout and very few if any images or advertisement. The language is standard mid-century French. Most issues are about 60 pages long, which fits the context window of the model. The scans are in a high resolution. The model can be prompted to perform specific transformations such as making the paragraph breaks but not line breaks and joining hyphenated words back into full words. All this makes text extraction easier.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "At the same time there are some challenges inherent in prompting a Large Language Model (LLM) for OCR. These includes issues related to the size of the PDFs, difficulties with verifying the quality and the completeness the output, and differences between prompting approaches. \n",
    "\n",
    "**PDF Size**\n",
    "\n",
    "the PDFs uploaded to the Gemini API cannot be larger than [50MB](https://github.com/googleapis/python-genai/issues/308). We had a total of 42 PDFs in this corpus and 18 of them were over 50MB. Some were only marginally larger, others were combined issues, reaching over 120 pages and 100MB. We could not split the PDFs automatically by size since our goal was to capture each article separately. A size or page number based approach could split up an article in the middle. This meant that we had to either reduce the size of the PDFs and risk reducing the quality of the resolution or split them manually. \n",
    "\n",
    "We had a two-part approach. First we transcribed all the PDFs, skipping the ones that were too large and making a note about that in the process. Afterwards we manually split the 18 large PDFs in Adobe Acrobat Pro, creating 42 new PDFs. We retained the filenames, adding '-A', '-B', etc to the end of the filenames. Then we ran a second pass of Gemini 2.5 transcription with the split PDFs.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../img/large_pdfs.png\"\n",
    "         alt=\"Screenshot showing the transcription token usage for API calls\">\n",
    "    <figcaption>Transcription token usage after the first pass</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../img/second_pass_pdfs.png\"\n",
    "         alt=\"Screenshot showing the transcription token usage for API calls\">\n",
    "    <figcaption>Transcription token usage after the second pass</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d6d19",
   "metadata": {},
   "source": [
    "**Quality and Completeness of the Output**\n",
    "\n",
    "It is difficult to verify that the entire PDF has been OCR'ed. Sometimes the model stopped producing text because it ran out of output tokens, particularly if the PDF was really long or if the model was using too many 'thinking' tokens. We retained the token usage for each PDF and looked for outliers. As you can see in the transcription usage table above, the PDF for June 1966 (163122_2-1966-06.pdf) has only 33 output tokens, which is only a fraction of the output tokens for the April 1966 issue which was 44k tokens long. In this case, this was not a model error but actually this issue does not exist. It has never been published. The PDF scanned by [the BAnQ](https://collections.banq.qc.ca/ark:/52327/2314811) reads \"Parti Pris Juin à Août Non paru\" (Parti Pris, June to August not published). In other instances like in September 1964 (163122_2-1964-09-01.pdf) and December 1964 (163122_2-1964-12-01.pdf), there are additional short pamphlets alongside the main issue. These pamphlets are denotated with '-01' after the date in the PDFs' filenames by the BAnQ. While these examples were real cases where the output should have way fewer tokens, this method of checking the output token sizes allowed us to run a simple analysis to discover inconsistencies in OCR output and rerun the API calls on issues were there was missing texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2607b543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pdf_filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "json_filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "thoughts_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "output_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e0bdba7e-3f9a-43e9-a918-e5fa870616f2",
       "rows": [
        [
         "0",
         "163122_1-1963-10.pdf",
         "163122_1-1963-10.json",
         "17107",
         "1406",
         "34382",
         "52895"
        ],
        [
         "1",
         "163122_1-1963-11.pdf",
         "163122_1-1963-11.json",
         "17107",
         "1456",
         "29757",
         "48320"
        ],
        [
         "2",
         "163122_1-1963-12.pdf",
         "163122_1-1963-12.json",
         "17107",
         "1986",
         "32445",
         "51538"
        ],
        [
         "3",
         "163122_1-1964-01.pdf",
         "163122_1-1964-01.json",
         "17107",
         "1381",
         "38662",
         "57150"
        ],
        [
         "4",
         "163122_1-1964-02.pdf",
         "163122_1-1964-02.json",
         "17107",
         "1371",
         "42279",
         "60757"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_filename</th>\n",
       "      <th>json_filename</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>thoughts_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163122_1-1963-10.pdf</td>\n",
       "      <td>163122_1-1963-10.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1406</td>\n",
       "      <td>34382</td>\n",
       "      <td>52895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163122_1-1963-11.pdf</td>\n",
       "      <td>163122_1-1963-11.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1456</td>\n",
       "      <td>29757</td>\n",
       "      <td>48320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163122_1-1963-12.pdf</td>\n",
       "      <td>163122_1-1963-12.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1986</td>\n",
       "      <td>32445</td>\n",
       "      <td>51538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163122_1-1964-01.pdf</td>\n",
       "      <td>163122_1-1964-01.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1381</td>\n",
       "      <td>38662</td>\n",
       "      <td>57150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163122_1-1964-02.pdf</td>\n",
       "      <td>163122_1-1964-02.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1371</td>\n",
       "      <td>42279</td>\n",
       "      <td>60757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pdf_filename          json_filename  prompt_tokens  \\\n",
       "0  163122_1-1963-10.pdf  163122_1-1963-10.json          17107   \n",
       "1  163122_1-1963-11.pdf  163122_1-1963-11.json          17107   \n",
       "2  163122_1-1963-12.pdf  163122_1-1963-12.json          17107   \n",
       "3  163122_1-1964-01.pdf  163122_1-1964-01.json          17107   \n",
       "4  163122_1-1964-02.pdf  163122_1-1964-02.json          17107   \n",
       "\n",
       "   thoughts_tokens  output_tokens  total_tokens  \n",
       "0             1406          34382         52895  \n",
       "1             1456          29757         48320  \n",
       "2             1986          32445         51538  \n",
       "3             1381          38662         57150  \n",
       "4             1371          42279         60757  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/transcription_usage_fulltext.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7fbb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "base_filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "thoughts_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "output_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "json_filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "note",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e22c394a-ee11-471d-9456-ee83a56203a0",
       "rows": [
        [
         "0",
         "163122_1-1963-10.json",
         "17107",
         "1406",
         "34382",
         "52895",
         "163122_1-1963-10.json",
         "163122_1-1963-10.pdf",
         ""
        ],
        [
         "1",
         "163122_1-1963-11.json",
         "17107",
         "1456",
         "29757",
         "48320",
         "163122_1-1963-11.json",
         "163122_1-1963-11.pdf",
         ""
        ],
        [
         "2",
         "163122_1-1963-12.json",
         "17107",
         "1986",
         "32445",
         "51538",
         "163122_1-1963-12.json",
         "163122_1-1963-12.pdf",
         ""
        ],
        [
         "3",
         "163122_1-1964-01.json",
         "17107",
         "1381",
         "38662",
         "57150",
         "163122_1-1964-01.json",
         "163122_1-1964-01.pdf",
         ""
        ],
        [
         "4",
         "163122_1-1964-02.json",
         "17107",
         "1371",
         "42279",
         "60757",
         "163122_1-1964-02.json",
         "163122_1-1964-02.pdf",
         ""
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_filename</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>thoughts_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>json_filename</th>\n",
       "      <th>pdf_filename</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163122_1-1963-10.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1406</td>\n",
       "      <td>34382</td>\n",
       "      <td>52895</td>\n",
       "      <td>163122_1-1963-10.json</td>\n",
       "      <td>163122_1-1963-10.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163122_1-1963-11.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1456</td>\n",
       "      <td>29757</td>\n",
       "      <td>48320</td>\n",
       "      <td>163122_1-1963-11.json</td>\n",
       "      <td>163122_1-1963-11.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163122_1-1963-12.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1986</td>\n",
       "      <td>32445</td>\n",
       "      <td>51538</td>\n",
       "      <td>163122_1-1963-12.json</td>\n",
       "      <td>163122_1-1963-12.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163122_1-1964-01.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1381</td>\n",
       "      <td>38662</td>\n",
       "      <td>57150</td>\n",
       "      <td>163122_1-1964-01.json</td>\n",
       "      <td>163122_1-1964-01.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163122_1-1964-02.json</td>\n",
       "      <td>17107</td>\n",
       "      <td>1371</td>\n",
       "      <td>42279</td>\n",
       "      <td>60757</td>\n",
       "      <td>163122_1-1964-02.json</td>\n",
       "      <td>163122_1-1964-02.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           base_filename  prompt_tokens  thoughts_tokens  output_tokens  \\\n",
       "0  163122_1-1963-10.json          17107             1406          34382   \n",
       "1  163122_1-1963-11.json          17107             1456          29757   \n",
       "2  163122_1-1963-12.json          17107             1986          32445   \n",
       "3  163122_1-1964-01.json          17107             1381          38662   \n",
       "4  163122_1-1964-02.json          17107             1371          42279   \n",
       "\n",
       "   total_tokens          json_filename          pdf_filename note  \n",
       "0         52895  163122_1-1963-10.json  163122_1-1963-10.pdf       \n",
       "1         48320  163122_1-1963-11.json  163122_1-1963-11.pdf       \n",
       "2         51538  163122_1-1963-12.json  163122_1-1963-12.pdf       \n",
       "3         57150  163122_1-1964-01.json  163122_1-1964-01.pdf       \n",
       "4         60757  163122_1-1964-02.json  163122_1-1964-02.pdf       "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the -A, -B, -C, etc. from filenames to group them\n",
    "df['base_filename'] = df['json_filename'].str.replace(r'-[A-Z]\\.json$', '.json', regex=True)\n",
    "\n",
    "# List of columns to sum\n",
    "token_cols = ['prompt_tokens', 'thoughts_tokens', 'output_tokens', 'total_tokens']\n",
    "\n",
    "# Group by base_filename, sum token columns, and aggregate other columns as needed\n",
    "df_combined = df.groupby('base_filename', as_index=False).agg(\n",
    "    {col: 'sum' for col in token_cols} | {\n",
    "        'json_filename': lambda x: ', '.join(sorted(set(x))),\n",
    "        'pdf_filename': lambda x: ', '.join(sorted(set(x)))\n",
    "    }\n",
    ")\n",
    "\n",
    "# Count how many original rows were grouped for each base_filename\n",
    "counts = df.groupby('base_filename').size().reset_index(name='count')\n",
    "\n",
    "# Merge counts into df_combined\n",
    "df_combined = df_combined.merge(counts, on='base_filename')\n",
    "\n",
    "# Add note only where count > 1\n",
    "df_combined['note'] = df_combined['count'].apply(lambda x: 'combined values' if x > 1 else '')\n",
    "\n",
    "# Optionally drop the 'count' column if you don't need it\n",
    "df_combined = df_combined.drop(columns=['count'])\n",
    "\n",
    "# Save or display the result\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6d9bd",
   "metadata": {},
   "source": [
    "## Gemini API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f1176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cesta",
   "language": "python",
   "name": "cesta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
